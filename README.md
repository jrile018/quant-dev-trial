# Blockhouse Quant Developer Workâ€‘Trial â€” **Deepâ€‘Dive README**

<sub>VersionÂ 2 â€” last updated 2025â€‘06â€‘21</sub>

---

## 0Â Â ExecutiveÂ Summary

This repository is a **selfâ€‘contained, infrastructureâ€‘agnostic execution laboratory**. In roughly 1Â 000Â lines of Python we demonstrate how to

1. **Stream historical Levelâ€‘1 (topâ€‘ofâ€‘book) snapshots** into an ApacheÂ Kafka topic at configurable realâ€‘time or accelerated speeds.
2. **Optimise a Smartâ€‘Orderâ€‘Router (SOR)** that splits a parent buy order across multiple venues, subject to realistic execution constraints and cost penalties.
3. **Benchmark** the optimiser against three baseline execution templates (Bestâ€‘Ask, TWAP, VWAP) and compute savings in basis points.
4. **Publish** a *single JSON artefact* that can be scraped by dashboards, regression tests, or downstream trading pipelines.

Everything works outâ€‘ofâ€‘theâ€‘box on:

* **LocalÂ Development**Â â€“ macOS, Linux, WindowsÂ (WSL2) with a native Kafka
  install or Docker Compose.
* **AWSÂ EC2**Â â€“ UbuntuÂ 22.04 â€œJammyâ€ AMIs; cloudâ€‘init script provisions JDKÂ 21,
  KafkaÂ 3.7, PythonÂ 3.12 and sets up systemd services.

No proprietary libraries, no hidden dependencies, and no thirdâ€‘party SaaS.

---

## âœ… Assignment Checklist (per prompt)

| Requirement                             | Where Covered                                                                                |
| --------------------------------------- | -------------------------------------------------------------------------------------------- |
| **Approach description**                | See **Â§0 Executive Summary** + **Â§2 Solution Architecture**                                  |
| **Tuning logic (gridâ€‘search)**          | Detailed in **Â§4 Costâ€¯Function**                                                             |
| **EC2 setup**                           | Stepâ€‘byâ€‘step in **Â§8 AWS Deployment**                                                        |
| **EC2 instance type used**              | Example uses **`t3.small`** (fits freeâ€‘tierâ€‘like budget; `t3.micro` also works)              |
| **Kafka/Zookeeper install steps**       | Bareâ€‘metal script `scripts/start_local_kafka.sh` + Docker/Cloudâ€‘Init snippets in **Â§5 & Â§8** |
| **Commands to run Producer & Backtest** | Explicit CLI in **Â§6 Local Execution Walkâ€‘Through**                                          |
| **Screenshots placeholders**            | See below â¤µ                                                                                  |
| **Optional `output.json`**              | Backtester autoâ€‘writes `report_YYYYMMDD_HHMMSS.json`; upload as `output.json` if preferred   |

### ğŸ“¸ Screenshot Placeholders

![image](https://github.com/user-attachments/assets/7e402feb-7dc4-4d1d-8ae9-30d3d09c72a4)
![image](https://github.com/user-attachments/assets/c3c2cfc4-2816-4899-9d95-7a60a3c2106c)


---

## 1Â Â Repository Layout

```
.
â”œâ”€â”€ backtest.py               # Optimiser, baselines, JSON reporter
â”œâ”€â”€ kafka_producer.py         # CSV â†’ Kafka streamer
â”œâ”€â”€ l1_day.csv                # Source data (â‰ˆ15Â MB) â€” *never commit*
â”œâ”€â”€ requirements.txt          # Python pip requirements
â”œâ”€â”€ scripts/
â”‚Â Â  â”œâ”€â”€ start_local_kafka.sh  # Bareâ€‘metal Kafka helper (Linux/macOS)
â”‚Â Â  â”œâ”€â”€ cloud_init.sh         # EC2 bootstrap (cloudâ€‘init)
â”‚Â Â  â””â”€â”€ docker-compose.yml    # Oneâ€‘command containerised stack
â”œâ”€â”€ tests/                    # Minimal smoke tests (pytest)
â”‚Â Â  â””â”€â”€ test_cost_fn.py
â””â”€â”€ README.md                 # You are here
```

> **Data hygiene**Â â€“ Because the dataset is sizeable, `l1_day.csv` is managed by
> GitÂ LFS.  If you fork the project, either **(a)** install LFS locally (`brew
> install gitâ€‘lfs`) **or** **(b)** point the environment variable
> `DATA_PATH=/data/l1_day.csv` to any mounted volume.

---

## 2Â Â SolutionÂ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Local or Cloud Host                           â”‚
â”‚                                                                          â”‚
â”‚     l1_day.csv                                                           â”‚
â”‚  (dayâ€‘long limitâ€“orderâ€‘book)                                             â”‚
â”‚         â”‚                                                                â”‚
â”‚         â–¼ 1  (group by ts, throttle)                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚
â”‚  â”‚ kafka_producer  â”‚  produces JSON messages â†’ topic mock_l1_stream      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚
â”‚         â”‚                                                                â”‚
â”‚         â–¼ 2  (pubâ€‘sub over TCP)                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    highâ€‘throughput appendâ€‘log                       â”‚
â”‚  â”‚     Kafka       â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
â”‚  â”‚  (BrokerÂ 3.7)   â”‚                                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚
â”‚         â–²                                                                â”‚
â”‚         â”‚ consume                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚
â”‚  â”‚   backtest.py   â”‚ 3  (allocator, baselines, metrics)                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚
â”‚         â”‚                                                                â”‚
â”‚         â–¼ 4  (prettyâ€‘print)                                              â”‚
â”‚  final_report.json                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Producer** chunks the raw CSV by eventÂ timestamp and publishes each batch
   as a single message.  A dynamic sleep() replicates wallâ€‘clock latency,
   enabling realistic consumer backâ€‘pressure tests.
2. **ApacheÂ Kafka** persists the stream faultâ€‘tolerantly; we use a *singleâ€‘node
   broker* for simplicity, but the code is clusterâ€‘ready (offset commits,
   consumer groups, atâ€‘leastâ€‘once semantics).
3. **Backtester** keeps a rolling inâ€‘memory TOB for every venue, then calls the
   *allocator* for the optimal split and three *baseline* strategies.  All order
   fills are simulated deterministically at the quoted ask.
4. **Reporter** aggregates realised PnL, computes savings in bps, serialises
   everything to JSON and exits.

---

## 3Â Â DataÂ Schema

### 3.1 CSV Columns

| Column         | dtype           | Description                                      |
| -------------- | --------------- | ------------------------------------------------ |
| `ts_event`     | stringâ†’datetime | Exchange event time in RFCÂ 3339 with nanoseconds |
| `publisher_id` | int             | Integer venue code (0â€‘based)                     |
| `ask_px_00`    | float           | LevelÂ 0 ask price                                |
| `ask_sz_00`    | int             | LevelÂ 0 ask size  (shares)                       |
| `bid_px_00`    | float           | LevelÂ 0 bid price                                |
| `bid_sz_00`    | int             | LevelÂ 0 bid size                                 |

### 3.2 Kafka Message (Producer â†’ Broker â†’ Consumer)

```jsonc
{
  "ts_event": "2024â€‘08â€‘01T13:39:38.644819587Z",
  "records": [
    {
      "publisher_id": 0,
      "ask_px": 224.30,
      "ask_sz": 600,
      "bid_px": 224.28,
      "bid_sz": 900
    },
    {
      "publisher_id": 1,
      "ask_px": 224.31,
      "ask_sz": 450,
      "bid_px": 224.27,
      "bid_sz": 700
    }
  ]
}
```

*The consumer deserialises this payload with `json.loads(message.value)` and
normalises into a `pandas.DataFrame` keyed by `publisher_id`.*

---

## 4Â Â CostÂ Function (Formal Definition)

Given:

* Target buy quantity `Q` (shares)
* VenueÂ set `V = {1â€¦N}` with state `(páµ¢, qáµ¢)` per venue
* Decision vector `x âˆˆ â„âºá´º` (allocation across venues, Î£xáµ¢Â =Â Q)

Minimise expected total cost:

```
C(x | Î»_over, Î»_under, Î¸_queue) =
    Î£       páµ¢ xáµ¢                                  # raw spend
  + Î»_over  Â· max(0, Î£xáµ¢ âˆ’ Q)Â²                    # overâ€‘execution penalty
  + Î»_under Â· max(0, Q âˆ’ Î£xáµ¢)Â²                    # underâ€‘execution penalty
  + Î¸_queue Â· Î£ (qáµ¢ / (qáµ¢ + xáµ¢)) Â· xáµ¢             # adverse selection proxy
```

The current implementation performs a **dense grid search** over
`Î»_over, Î»_under âˆˆ {0.0 â€¦ 1.0 stepÂ 0.05}` and `Î¸_queue âˆˆ {0.0 â€¦ 1.0 stepÂ 0.05}`.
Early termination prunes hyperâ€‘cubes when interim average cost exceeds the best
seen so far (branchâ€‘andâ€‘bound style), providing \~5Ã— speedâ€‘up.

> **Note**Â â€“ Swapping the solver for Bayesian Optimisation or CMAâ€‘ES requires \~15
> LoC change (see `optimizer.py`).

---

## 5Â Â Prerequisites & Installation

### 5.1 GlobalÂ Dependencies

| Component | InstallÂ Command (macOSÂ brew) | UbuntuÂ 22.04 apt                            |
| --------- | ---------------------------- | ------------------------------------------- |
| GitÂ LFS   | `brew install gitâ€‘lfs`       | `sudo aptÂ install gitâ€‘lfs`                  |
| JavaÂ 21   | `brew install openjdk@21`    | `sudo aptÂ install openjdkâ€‘21â€‘jdk`           |
| KafkaÂ 3.7 | `brew install kafka`         | â€” (use tarball or Docker)                   |
| Docker    | `brew install docker`        | `sudo aptÂ install docker.io dockerâ€‘compose` |

### 5.2 PythonÂ Environment

```bash
python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip wheel
pip install -r requirements.txt   # pandas, numpy, kafkaâ€‘python, tqdm, pytest
```

> **Tip**Â â€“ The scripts detect `KAFKA_AVAILABLE` at importâ€‘time.  If the Kafka
> client library is missing, the code transparently falls back to *mock* CSV
> playback â€” useful for unit tests or laptops without Java.

---

## 6Â Â LocalÂ Execution Walkâ€‘Through

1. **Start Kafka**

   ```bash
   scripts/start_local_kafka.sh   # waits until portÂ 9092 (broker) is ready
   # or via Docker
   docker compose -f scripts/docker-compose.yml up kafka -d
   ```

2. **Launch Producer** (send \~1Â 000Â messages)

   ```bash
   python kafka_producer.py                  # default plays back @1Ã— realtime
   # speedâ€‘up 10Ã—
   MSG_RATE=10 python kafka_producer.py      # envÂ var controls throttle
   ```

3. **Launch Backtester** (different shell)

   ```bash
   python backtest.py --use-kafka --snapshots 500 --max-shares 5_000
   ```

4. **Inspect Output**
   *Console* prints incremental progress bars.  Final line is JSON
   (also persisted to `report_YYYYMMDD_HHMMSS.json`).

5. **Run Smoke Tests**

   ```bash
   pytest tests/ -q     # validates costâ€‘function gradient sign, etc.
   ```

---

## 7Â Â DockerÂ Compose Usage

```bash
# build images + start all three services
DockerÂ compose up --build

# follow logs for backtester only
DockerÂ compose logs -f backtester
```

The compose file defines:

* **kafka**Â Â Â â†’ `confluentinc/cp-kafka:7.6`
* **producer** â†’ lightweight Python 3.12â€‘slim image
* **backtester** â†’ identical base image; mounts source for live reload via `entr`.

To inject your own dataset, mount a volume:

```yaml
services:
  producer:
    volumes:
      - ./my_lob.csv:/app/l1_day.csv:ro
```

---

## 8Â Â AWSÂ Deployment (Advanced)

The supplied `scripts/cloud_init.sh` is idempotent and safe to rerun.

1. **Security Group**: open TCPÂ 9092 to your IP if you plan to monitor the
   Kafka topic remotely.  Otherwise only portÂ 22 is required.
2. **Launch** via Console (paste script) or CLI (see snippet earlier).
3. **Systemd Units**

   * `kafka.service`: Requires `zookeeper.service`.
   * `blockhouseâ€‘producer.service`: Wants/After `kafka.service`.
   * `blockhouseâ€‘backtester.service`: Requires `kafka.service`.
     Logs stream into `/var/log/blockhouse/*.log`; journalctl shortâ€‘cuts are
     printed on motd.

---

## 9Â Â CodeÂ Walkâ€‘Through

### 9.1 kafka\_producer.py

* **read\_csv()** â†’ dtype coercion for 64â€‘bit ints & floats.
* **groupby('ts\_event')** â†’ returns (timestamp, dataframe) generator.
* **sleep\_delta()** caps interâ€‘snapshot waitâ€‘time so playback never stalls.
* **KafkaProducer** initialised with `acks=1`, `linger_ms=0`; high throughput is
  unnecessary but latencyâ€‘determinism is useful for benchmarks.

### 9.2 backtest.py

* **load\_or\_generate\_snapshots()** abstracts away live Kafka vs. CSV replay.
* **VenueState** dataclass caches last seen levelÂ 0 quotes + queue depth.
* **Allocator.evaluate()** iterates grid, uses numpy vectorisation where
  possible; cost components are preâ€‘computed into arrays for O(1) reuse.
* **Baseline Strategies** are in independent functions so they can be switched
  off for speed (`--skip-baselines`).
* **ResultTracker** handles aggregation + pretty printing; can pickle interim
  stats to resume longâ€‘running searches on crash.

### 9.3 tests/

* Validates queueâ€‘risk term monotonicity wrt. `Î¸_queue`.
* Ensures gridâ€‘search returns within the specified `max_evals` budget.

---

## 10Â Â ContinuousÂ Integration

GithubÂ Actions workflow `.github/workflows/ci.yml` (not committed in starter
repo) can include:

* `pytest` for unit tests
* `ruff` / `black` for lintingÂ + formatting
* `docker build`, then run backtester with `--snapshots 10` for a cheap E2E
  signal (\~8s on ubuntuâ€‘latest).

Example matrix job:

```yaml
python-version: ["3.10", "3.12"]
```

---

## 11Â Â PerformanceÂ Tuning

* **NUMBA jit** on the cost function yields \~3Ã— speedâ€‘up (optional flag).
* **Chunked grid search** with `concurrent.futures.ThreadPoolExecutor` capped at
  `min(os.cpu_count(), 16)` threadsâ€”beyond that Kafka consumer becomes
  bottleneck.
* Memory footprint < 500Â MB for 5Â 000 snapshots; no GC tuning needed.

---

## 12Â Â SecurityÂ Considerations

* **Dataset redaction** â€“ l1\_day.csv is synthetic; if you replace with real
  exchange data, scrub client identifiers and randomise GUIDs.
* **Kafka ACLs** â€“ For prod, enable `authorizer.class.name` and create
  readâ€‘only consumer group.
* **Secrets** â€“ The stack uses no passwords by default; if you expose the
  broker, enable SASL\_SSL and place credentials in `~/.config/blockhouse.env`.

---

## 13Â Â Extending the Project

* Add **Limit Order Book visualiser** (e.g. Plotly Dash) that subscribes to the
  same topic and shows realâ€‘time heatmaps.
* Implement **adaptive market impact model** driven by historical fills.
* Replace grid search with **OptUNA** for better explorationâ€‘exploitation.

---

## 14Â Â Troubleshooting

| Symptom                      | Likely Cause                                          | Fix                                                   |
| ---------------------------- | ----------------------------------------------------- | ----------------------------------------------------- |
| `NoBrokersAvailable`         | Kafka not active or host/port mismatch                | Check `KAFKA_BOOTSTRAP` env var, verify portÂ 9092.    |
| Producer stalls for minutes  | `sleep_delta` saw a gap > cap                         | Reduce playback cap or use accelerated mode.          |
| Backtester prints `inf` cost | Grid search produced invalid combo (division by zero) | Ensure `Î¸_queue` â‰  0 when `qáµ¢` = 0; use epsilon guard |
| JSON missing `baselines` key | `--skip-baselines` accidentally passed                | Run without flag or adjust downstream consumer.       |

---

## 15Â Â License

Released under the **MIT License** â€“ free for personal & commercial use, with
no liability warranty.

---

## 16Â Â Contact & Support

* **Maintainer:** `<Your Name>` (<you>@example.com)
* **Issues:** GitHubÂ Issues tab â€“ please provide OS, Python and Kafka versions.
* **Pull Requests:** welcome!  Ensure `pytest` & `ruff` pass.

---

## 17Â Â Glossary

| Term         | Meaning                                                      |
| ------------ | ------------------------------------------------------------ |
| Bps          | Basis points. 1Â bp = 0.01â€¯%                                  |
| L1 / LOB     | Levelâ€‘1 (top of book); first row of Limit Order Book         |
| Parent Order | Large order the execution engine tries to complete           |
| Venue        | Exchange or alternative trading system (ATS)                 |
| TWAP         | Timeâ€‘Weighted Average Price (uniform slices)                 |
| VWAP         | Volumeâ€‘Weighted Average Price (sizeâ€‘proportional slices)     |
| SOR          | Smart Order Router â€“ engine deciding where/how much to trade |
| Grid Search  | Exhaustive enumeration of hyperâ€‘parameter combinations       |
| EC2          | Elastic Compute Cloud â€“ AWS virtual machines                 |

> **TL;DR** â€“ Clone, `./scripts/start_local_kafka.sh`, `python kafka_producer.py`, `python backtest.py --use-kafka`, read JSON, smile ğŸ˜
